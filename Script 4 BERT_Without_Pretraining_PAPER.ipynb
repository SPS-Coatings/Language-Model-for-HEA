{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard  (optional)\n",
    "#sto comand prompt\n",
    "#taskkill /im tensorboard.exe /f\n",
    "#del /q %TMP%\\.tensorboard-info\\*\n",
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir runs/train\n",
    "%tensorboard --logdir ./logs\n",
    "#%tensorboard --logdir {logs_base_dir}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52d803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdba31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################### The script assumes that the target property is hardness but the values have been replaced for UTS in the dataset ##########################\n",
    "\n",
    "import warnings  # For suppressing warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress all warning messages\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "                          AdamW, get_linear_schedule_with_warmup, TrainerCallback, EarlyStoppingCallback)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Import Seaborn\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import re\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# ==========================================\n",
    "# 1. Parameters and Setup\n",
    "# ==========================================\n",
    "\n",
    "# Parameters\n",
    "K = 5  # Number of folds\n",
    "random_seed = 42\n",
    "output_base_dir = './'  # Specify your saving directory here\n",
    "\n",
    "# Create base directory if it doesn't exist\n",
    "if not os.path.exists(output_base_dir):\n",
    "    os.makedirs(output_base_dir)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Loading and Preprocessing\n",
    "# ==========================================\n",
    "\n",
    "# Load the data\n",
    "#data = pd.read_csv('LLM_Elongation_features_Rounded_Cleaned.csv')\n",
    "data = pd.read_csv('LLM_UTS_Features.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_tokenize(composition):\n",
    "    \"\"\"\n",
    "    Custom tokenizer to parse the composition string into sorted element-fraction tokens.\n",
    "    Example:\n",
    "        Input: \"Co1.2 F0.8 Ni1\"\n",
    "        Output: ['Co1.2', 'F0.8', 'Ni1']\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'([A-Z][a-z]*)([0-9.]+)', composition)\n",
    "    # Sort matches by the element's name to ensure alphabetical order\n",
    "    sorted_matches = sorted(matches, key=lambda x: x[0])\n",
    "    tokens = []\n",
    "    for match in sorted_matches:\n",
    "        element, fraction = match\n",
    "        token = f\"{element}{fraction}\"  # Combine element and fraction\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "# Test the function\n",
    "print(custom_tokenize(\"Co1.2 F0.8 Ni1\"))  # Output should be: ['Co1.2', 'F0.8', 'Ni1']\n",
    "\n",
    "# Create tokenized_elements column by applying custom_tokenize on the composition column\n",
    "data['tokenized_elements'] = data['composition'].apply(custom_tokenize)\n",
    "\n",
    "# ==========================================\n",
    "# 3. K-Fold Cross-Validation Setup\n",
    "# ==========================================\n",
    "\n",
    "# Initialize K-Fold\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=random_seed)\n",
    "\n",
    "# Prepare to store metrics for each fold\n",
    "fold_metrics = {\n",
    "    'mse': [],\n",
    "    'mae': [],\n",
    "    'r2': []\n",
    "}\n",
    "\n",
    "# Prepare to collect predictions and actual values across folds\n",
    "all_predictions_unscaled = []\n",
    "all_actual_values_unscaled = []\n",
    "\n",
    "# Define the CustomDataset class outside the K-Fold loop\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ==========================================\n",
    "# 4. K-Fold Cross-Validation Loop\n",
    "# ==========================================\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(data)):\n",
    "    print(f\"\\n########### Fold {fold + 1} / {K} ###########\")\n",
    "    \n",
    "    # Split data\n",
    "    data_train = data.iloc[train_index].reset_index(drop=True)\n",
    "    data_val = data.iloc[val_index].reset_index(drop=True)\n",
    "    \n",
    "    # Normalize 'hardness' column within the fold\n",
    "    hardness_scaler = StandardScaler()\n",
    "    data_train['normalized_hardness'] = hardness_scaler.fit_transform(data_train[['hardness']])\n",
    "    data_val['normalized_hardness'] = hardness_scaler.transform(data_val[['hardness']])\n",
    "\n",
    "    # Initialize a dictionary to store all scalers for this fold\n",
    "    scalers = {'hardness_scaler': hardness_scaler}\n",
    "    \n",
    "    # Normalize all other numeric features within the fold and store their scalers\n",
    "    feature_columns = [col for col in data.columns if col not in ['composition', 'hardness', 'tokenized_elements']]\n",
    "    # Initialize combined features for train and validation\n",
    "    combined_features_train = data_train['tokenized_elements'].astype(str)\n",
    "    combined_features_val = data_val['tokenized_elements'].astype(str)\n",
    "\n",
    "    for feature in feature_columns:\n",
    "        if feature not in ['composition', 'hardness', 'tokenized_elements']:\n",
    "            scaler = StandardScaler()\n",
    "            data_train[f'normalized_{feature}'] = scaler.fit_transform(data_train[[feature]])\n",
    "            data_val[f'normalized_{feature}'] = scaler.transform(data_val[[feature]])\n",
    "            combined_features_train += ' ' + data_train[f'normalized_{feature}'].astype(str)\n",
    "            combined_features_val += ' ' + data_val[f'normalized_{feature}'].astype(str)\n",
    "            \n",
    "            # Add the scaler to the dictionary with a unique key\n",
    "            scalers[f'{feature}_scaler'] = scaler\n",
    "    \n",
    "    # Assign the combined features back to the dataframe\n",
    "    data_train['combined_features'] = combined_features_train\n",
    "    data_val['combined_features'] = combined_features_val\n",
    "    \n",
    "    # Save all scalers for this fold using joblib\n",
    "    save_dir = os.path.join(output_base_dir, f'fold_{fold + 1}')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    scalers_save_path = os.path.join(save_dir, 'scalers.pkl')\n",
    "    joblib.dump(scalers, scalers_save_path)\n",
    "    print(f\"Scalers saved to {scalers_save_path}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. Model Initialization and Tokenization\n",
    "    # ==========================================\n",
    "\n",
    "    # Initialize tokenizer and model for each fold\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # Update as needed 150K_With_Hardness, 6K_pretraining\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    # Tokenize training data\n",
    "    train_encodings = tokenizer(\n",
    "        [\" \".join(tokens) for tokens in data_train['tokenized_elements'].to_list()],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Tokenize validation data\n",
    "    val_encodings = tokenizer(\n",
    "        [\" \".join(tokens) for tokens in data_val['tokenized_elements'].to_list()],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(train_encodings, data_train['normalized_hardness'].values)\n",
    "    val_dataset = CustomDataset(val_encodings, data_val['normalized_hardness'].values)\n",
    "    \n",
    "    def compute_metrics(p):\n",
    "        \"\"\"\n",
    "        Compute evaluation metrics: MSE, MAE, R2.\n",
    "        \"\"\"\n",
    "        predictions, labels = p.predictions.squeeze(), p.label_ids\n",
    "        mse = mean_squared_error(labels, predictions)\n",
    "        mae = mean_absolute_error(labels, predictions)\n",
    "        r2 = r2_score(labels, predictions)\n",
    "        return {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    \n",
    "    # Unique log directory for Tensorboard for each fold\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    log_dir = f'./logs/fold_{fold + 1}_{current_time}'\n",
    "    \n",
    "    # Define optimizer groups for decay mechanism\n",
    "    decay_layers = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"]  # Adjust as necessary\n",
    "    decay_param_names = [n for n, p in model.named_parameters() if any(f\".{layer}.\" in n for layer in decay_layers)]\n",
    "    no_decay_param_names = [n for n, p in model.named_parameters() if n not in decay_param_names]\n",
    "    decay_params = [p for n, p in model.named_parameters() if n in decay_param_names]\n",
    "    no_decay_params = [p for n, p in model.named_parameters() if n in no_decay_param_names]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": decay_params, \"weight_decay\": 0.02},\n",
    "        {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=6e-5)  # lr=6e-5 best so far\n",
    "    num_training_steps = len(train_dataset) * 100  # 10 epochs\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 6. Training Arguments and Callbacks\n",
    "    # ==========================================\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(output_base_dir, f'fold_{fold + 1}'),\n",
    "        num_train_epochs=100,  \n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        logging_dir=log_dir,\n",
    "        logging_steps=1,\n",
    "        save_steps=1000,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=1,  # Ensure evaluation happens every step\n",
    "        load_best_model_at_end=True,\n",
    "        report_to='tensorboard',\n",
    "        seed=random_seed,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "    \n",
    "    # Create a callback to save metrics at each evaluation step\n",
    "    class MetricsCallback(TrainerCallback):\n",
    "        \"\"\"\n",
    "        Custom callback to store evaluation metrics at each step.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.metrics = []\n",
    "\n",
    "        def on_evaluate(self, args, state, control, **kwargs):\n",
    "            if state.log_history:\n",
    "                self.metrics.append(state.log_history[-1])\n",
    "    \n",
    "    metrics_callback = MetricsCallback()\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, lr_scheduler),  # Pass them here directly\n",
    "        callbacks=[metrics_callback]\n",
    "    )\n",
    "    \n",
    "    # ==========================================\n",
    "    # 7. Training and Evaluation\n",
    "    # ==========================================\n",
    "\n",
    "    # Training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save all step metrics to CSV\n",
    "    step_metrics = pd.DataFrame(metrics_callback.metrics)\n",
    "    step_metrics.to_csv(f'{save_dir}/fold_{fold + 1}_step_metrics.csv', index=False)\n",
    "    \n",
    "    # Find the best metric (e.g., lowest eval_mse)\n",
    "    best_metric = step_metrics.loc[step_metrics['eval_mse'].idxmin()]\n",
    "    print(f\"Best Step Metrics for Fold {fold + 1}:\")\n",
    "    print(best_metric)\n",
    "    \n",
    "    # Save best metric to a CSV file\n",
    "    best_metric.to_frame().transpose().to_csv(f'{save_dir}/fold_{fold + 1}_best_metric.csv', index=False)\n",
    "    \n",
    "    # Evaluation\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Fold {fold + 1} Metrics: {results}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    fold_metrics['mse'].append(results['eval_mse'])\n",
    "    fold_metrics['mae'].append(results['eval_mae'])\n",
    "    fold_metrics['r2'].append(results['eval_r2'])\n",
    "    \n",
    "    # Save the fine-tuned model for the fold\n",
    "    trainer.save_model(os.path.join(output_base_dir, f'fold_{fold + 1}'))\n",
    "    \n",
    "    # Save tokenizer for the fold\n",
    "    tokenizer.save_pretrained(os.path.join(output_base_dir, f'fold_{fold + 1}'))\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    fold_results_df = pd.DataFrame([results])\n",
    "    fold_results_df.to_csv(f'{save_dir}/fold_{fold + 1}_metrics.csv', index=False)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 8. Visualization for the Fold\n",
    "    # ==========================================\n",
    "\n",
    "    # Visualize actual vs predicted for the fold\n",
    "    predictions = trainer.predict(val_dataset).predictions.squeeze()\n",
    "    actual_values = data_val['normalized_hardness'].values\n",
    "    \n",
    "    # Convert normalized target values back to original scale\n",
    "    actual_values_unscaled = hardness_scaler.inverse_transform(actual_values.reshape(-1, 1)).squeeze()\n",
    "    predictions_unscaled = hardness_scaler.inverse_transform(predictions.reshape(-1, 1)).squeeze()\n",
    "    \n",
    "    # Collect predictions and actual values for combined plots later\n",
    "    all_predictions_unscaled.extend(predictions_unscaled)\n",
    "    all_actual_values_unscaled.extend(actual_values_unscaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(x=actual_values_unscaled, y=predictions_unscaled, alpha=0.5)\n",
    "    plt.plot([actual_values_unscaled.min(), actual_values_unscaled.max()],\n",
    "             [actual_values_unscaled.min(), actual_values_unscaled.max()],\n",
    "             color='red', linestyle='--', label=\"Ideal Prediction\")\n",
    "    plt.xlabel(\"Actual UTS Values\", fontsize=15)\n",
    "    plt.ylabel(\"Predicted UTS Values\", fontsize=15)\n",
    "    plt.title(f\"Fold {fold + 1}: Actual vs Predicted UTS Values\", fontsize=18)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/actual_vs_predicted_UTS.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute residuals\n",
    "    residuals = actual_values_unscaled - predictions_unscaled\n",
    "    \n",
    "    # 1. Residual Box Plot with Individual Points using Seaborn\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(x=residuals, color='lightblue', fliersize=0)  # fliersize=0 hides outliers\n",
    "    sns.stripplot(x=residuals, color='green', alpha=0.5, size=4, jitter=True, label='Residuals')\n",
    "    plt.xlabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "    plt.title(f\"Fold {fold + 1}: Residuals Box Plot with Individual Points\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/residuals_box_plot_seaborn.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Residual Distribution Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Histogram\n",
    "    plt.hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black', density=True, label='Histogram')\n",
    "    \n",
    "    # Kernel Density Estimate (KDE)\n",
    "    sns.kdeplot(residuals, color='red', linewidth=2, label='KDE')\n",
    "    \n",
    "    plt.xlabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.title(f\"Fold {fold + 1}: Residuals Distribution\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/residuals_distribution.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Residuals vs Predicted Values Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=predictions_unscaled, y=residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Values\", fontsize=12)\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "    plt.title(f\"Fold {fold + 1}: Residuals vs. Predicted Values\", fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/residuals_vs_predicted.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Autocorrelation of Residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(residuals, lags=30, alpha=0.05)\n",
    "    plt.title(f\"Fold {fold + 1}: Autocorrelation of Residuals\", fontsize=14)\n",
    "    plt.xlabel(\"Lag\", fontsize=12)\n",
    "    plt.ylabel(\"Autocorrelation\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/autocorrelation_residuals.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "# ==========================================\n",
    "# 10. Aggregating Cross-Validation Results\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n########### Cross-Validation Results ###########\")\n",
    "for metric in fold_metrics:\n",
    "    scores = fold_metrics[metric]\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"{metric.upper()}: Mean = {mean_score:.4f}, Std = {std_score:.4f}\")\n",
    "\n",
    "# Optionally, save the aggregated metrics to a CSV\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "metrics_summary = metrics_df.agg(['mean', 'std']).transpose().reset_index()\n",
    "metrics_summary.columns = ['Metric', 'Mean', 'Std']\n",
    "metrics_summary.to_csv(os.path.join(output_base_dir, 'cross_validation_metrics.csv'), index=False)\n",
    "print(\"\\nCross-validation metrics saved to 'cross_validation_metrics.csv'\")\n",
    "\n",
    "# Convert collected predictions and actual values to numpy arrays\n",
    "all_predictions_unscaled = np.array(all_predictions_unscaled)\n",
    "all_actual_values_unscaled = np.array(all_actual_values_unscaled)\n",
    "\n",
    "# Compute overall metrics on combined data\n",
    "mse = mean_squared_error(all_actual_values_unscaled, all_predictions_unscaled)\n",
    "mae = mean_absolute_error(all_actual_values_unscaled, all_predictions_unscaled)\n",
    "r2 = r2_score(all_actual_values_unscaled, all_predictions_unscaled)\n",
    "print(\"\\n########### Overall Metrics on Combined Data ###########\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "# Save the overall metrics to a CSV file\n",
    "overall_metrics_df = pd.DataFrame({'MSE': [mse], 'MAE': [mae], 'R2': [r2]})\n",
    "overall_metrics_df.to_csv(os.path.join(output_base_dir, 'overall_metrics.csv'), index=False)\n",
    "print(\"\\nOverall metrics saved to 'overall_metrics.csv'\")\n",
    "\n",
    "# ==========================================\n",
    "# 11. Combined Visualization\n",
    "# ==========================================\n",
    "\n",
    "# 1. Combined Actual vs Predicted Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=all_actual_values_unscaled, y=all_predictions_unscaled, alpha=0.5)\n",
    "plt.plot([all_actual_values_unscaled.min(), all_actual_values_unscaled.max()],\n",
    "         [all_actual_values_unscaled.min(), all_actual_values_unscaled.max()],\n",
    "         color='red', linestyle='--', label=\"Ideal Prediction\")\n",
    "plt.xlabel(\"Actual UTS Values\", fontsize=15)\n",
    "plt.ylabel(\"Predicted UTS Values\", fontsize=15)\n",
    "plt.title(f\"Combined: Actual vs Predicted UTS Values\", fontsize=18)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_base_dir, 'combined_actual_vs_predicted_UTS.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Compute residuals\n",
    "combined_residuals = all_actual_values_unscaled - all_predictions_unscaled\n",
    "\n",
    "# 2. Combined Residual Box Plot with Individual Points\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=combined_residuals, color='lightblue', fliersize=0)\n",
    "sns.stripplot(x=combined_residuals, color='green', alpha=0.5, size=4, jitter=True, label='Residuals')\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "plt.title(\"Combined: Residuals Box Plot with Individual Points\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_base_dir, 'combined_residuals_box_plot_seaborn.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3. Combined Residual Distribution Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(combined_residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black', density=True, label='Histogram')\n",
    "sns.kdeplot(combined_residuals, color='red', linewidth=2, label='KDE')\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.title(\"Combined: Residuals Distribution\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_base_dir, 'combined_residuals_distribution.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 4. Combined Residuals vs Predicted Values Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=all_predictions_unscaled, y=combined_residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Values\", fontsize=12)\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\", fontsize=12)\n",
    "plt.title(\"Combined: Residuals vs. Predicted Values\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_base_dir, 'combined_residuals_vs_predicted.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 5. Combined Autocorrelation of Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(combined_residuals, lags=30, alpha=0.05)\n",
    "plt.title(\"Combined: Autocorrelation of Residuals\", fontsize=14)\n",
    "plt.xlabel(\"Lag\", fontsize=12)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_base_dir, 'combined_autocorrelation_residuals.png'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# # ==========================================\n",
    "# # End of Script\n",
    "# # ==========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Attention Map  ####################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "# Load your trained model and tokenizer\n",
    "model_path = './'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, output_attentions=True)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input text\n",
    "input_text = \"Co1 Cr1 Fe1 Mn1 Ni1 V1\"\n",
    "\n",
    "# Tokenize with offset mappings\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "offset_mapping = inputs['offset_mapping'][0]\n",
    "\n",
    "# Remove 'offset_mapping' from inputs before passing to the model\n",
    "model_inputs = {\n",
    "    'input_ids': inputs['input_ids'],\n",
    "    'attention_mask': inputs['attention_mask']\n",
    "}\n",
    "\n",
    "# Get Attention Weights\n",
    "outputs = model(**model_inputs)\n",
    "attentions = outputs.attentions  # List of attention weights for each layer\n",
    "\n",
    "# Aggregate across all heads for simplicity (could be refined)\n",
    "avg_attention = attentions[-1].squeeze(0).mean(0).detach().numpy()\n",
    "\n",
    "# Exclude the [CLS] and [SEP] tokens\n",
    "avg_attention = avg_attention[1:-1, 1:-1]\n",
    "offset_mapping = offset_mapping[1:-1]\n",
    "\n",
    "# Get tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])[1:-1]\n",
    "\n",
    "# Get components from input text\n",
    "components = input_text.strip().split()\n",
    "\n",
    "# Get component spans\n",
    "component_spans = []\n",
    "current_pos = 0\n",
    "for component in components:\n",
    "    start = input_text.index(component, current_pos)\n",
    "    end = start + len(component)\n",
    "    component_spans.append((start, end))\n",
    "    current_pos = end + 1  # Assuming one space between components\n",
    "\n",
    "# Map tokens to components\n",
    "token_to_component = []\n",
    "for token_idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "    token_start = token_start.item()\n",
    "    token_end = token_end.item()\n",
    "    # For each token, find which component it belongs to\n",
    "    for comp_idx, (comp_start, comp_end) in enumerate(component_spans):\n",
    "        # Check if token overlaps with component\n",
    "        if token_start >= comp_start and token_end <= comp_end:\n",
    "            token_to_component.append(comp_idx)\n",
    "            break\n",
    "    else:\n",
    "        # Token does not belong to any component\n",
    "        token_to_component.append(-1)\n",
    "\n",
    "# Map components to token indices\n",
    "component_to_token_indices = {}\n",
    "for token_idx, comp_idx in enumerate(token_to_component):\n",
    "    if comp_idx == -1:\n",
    "        continue\n",
    "    if comp_idx not in component_to_token_indices:\n",
    "        component_to_token_indices[comp_idx] = []\n",
    "    component_to_token_indices[comp_idx].append(token_idx)\n",
    "\n",
    "# Extract element symbols from components\n",
    "def extract_element(component):\n",
    "    match = re.match(r\"([A-Za-z]+)\", component)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return component\n",
    "\n",
    "component_elements = [extract_element(component) for component in components]\n",
    "\n",
    "# Map elements to component indices\n",
    "element_to_component_indices = {}\n",
    "for idx, elem in enumerate(component_elements):\n",
    "    if elem not in element_to_component_indices:\n",
    "        element_to_component_indices[elem] = []\n",
    "    element_to_component_indices[elem].append(idx)\n",
    "\n",
    "# Map elements to token indices\n",
    "element_to_token_indices = {}\n",
    "for elem, comp_indices in element_to_component_indices.items():\n",
    "    token_indices = []\n",
    "    for comp_idx in comp_indices:\n",
    "        token_indices.extend(component_to_token_indices.get(comp_idx, []))\n",
    "    element_to_token_indices[elem] = token_indices\n",
    "\n",
    "# Get unique elements\n",
    "unique_elements = list(element_to_token_indices.keys())\n",
    "n_elements = len(unique_elements)\n",
    "\n",
    "# Initialize reduced attention matrix\n",
    "reduced_attention = np.zeros((n_elements, n_elements))\n",
    "\n",
    "# Compute average attention for each element pair\n",
    "for i, elem_i in enumerate(unique_elements):\n",
    "    indices_i = element_to_token_indices[elem_i]\n",
    "    for j, elem_j in enumerate(unique_elements):\n",
    "        indices_j = element_to_token_indices[elem_j]\n",
    "        if indices_i and indices_j:\n",
    "            attention_values = avg_attention[np.ix_(indices_i, indices_j)]\n",
    "            reduced_attention[i, j] = attention_values.mean()\n",
    "        else:\n",
    "            reduced_attention[i, j] = 0\n",
    "\n",
    "# Make the attention matrix symmetric by averaging with its transpose\n",
    "symmetric_attention = (reduced_attention + reduced_attention.T) / 2\n",
    "\n",
    "# Mask the diagonal to exclude self-pairs\n",
    "np.fill_diagonal(symmetric_attention, 0)\n",
    "\n",
    "# Visualize\n",
    "sns.heatmap(\n",
    "    symmetric_attention,\n",
    "    annot=True,\n",
    "    xticklabels=unique_elements,\n",
    "    yticklabels=unique_elements,\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "plt.title(\"Symmetric Attention Map (Self-Pairs Masked)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090addd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
